{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bfd82ea",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "678706dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jennyraikakou/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/opt/anaconda3/envs/topic-modeling-env/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# nltk for the stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9cd427",
   "metadata": {},
   "source": [
    "# Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76e02590",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/ubs-mobile-app-reviews-clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2652227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1570, 13)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d452a311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0.1', 'Unnamed: 0', 'reviewId', 'userName', 'userImage',\n",
       "       'content', 'score', 'thumbsUpCount', 'reviewCreatedVersion', 'at',\n",
       "       'replyContent', 'repliedAt', 'clean_content'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24a9c724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>reviewId</th>\n",
       "      <th>userName</th>\n",
       "      <th>userImage</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>thumbsUpCount</th>\n",
       "      <th>reviewCreatedVersion</th>\n",
       "      <th>at</th>\n",
       "      <th>replyContent</th>\n",
       "      <th>repliedAt</th>\n",
       "      <th>clean_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>gp:AOqpTOFKM8vyKDl8bQv21U8i2O8m6EdIkpCn8XNWJYj...</td>\n",
       "      <td>yoann mii</td>\n",
       "      <td>https://play-lh.googleusercontent.com/a/AATXAJ...</td>\n",
       "      <td>doesnt work on many phones and full of bugs</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12.5.64086</td>\n",
       "      <td>2022-06-02 17:06:19</td>\n",
       "      <td>Thank you for your feedback &amp; please excuse th...</td>\n",
       "      <td>2022-06-03 13:51:16</td>\n",
       "      <td>doesnt work phones bugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>gp:AOqpTOGpGnFdSoTVSpYBE-XY0td_ZsoQX9lbL_aHZAc...</td>\n",
       "      <td>Pratik Gilda</td>\n",
       "      <td>https://play-lh.googleusercontent.com/a-/AOh14...</td>\n",
       "      <td>Update on 2-june-22: unable to login with acce...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12.5.64086</td>\n",
       "      <td>2022-06-02 16:06:12</td>\n",
       "      <td>Thank you very much for your patience and plea...</td>\n",
       "      <td>2022-06-03 14:52:29</td>\n",
       "      <td>update june unable login access app update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>gp:AOqpTOEPnZAw5fgoez35bU4IvWwSKSrfuWkwFs4USPK...</td>\n",
       "      <td>Radosław Kania</td>\n",
       "      <td>https://play-lh.googleusercontent.com/a/AATXAJ...</td>\n",
       "      <td>app is very slow. additional app for access in...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>12.5.64086</td>\n",
       "      <td>2022-06-01 22:39:43</td>\n",
       "      <td>Thank you for your feedback &amp; please excuse th...</td>\n",
       "      <td>2022-06-02 16:19:20</td>\n",
       "      <td>app slow additional app access shocker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>gp:AOqpTOEFeYQ8CVjsVT-13q2tNJWFbabIEiOupsx5hkb...</td>\n",
       "      <td>adi leist</td>\n",
       "      <td>https://play-lh.googleusercontent.com/a/AATXAJ...</td>\n",
       "      <td>Lots of bugs since the last release. 1) QR pay...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12.5.64086</td>\n",
       "      <td>2022-06-01 16:59:37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lots bugs release payments scanning function d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>gp:AOqpTOHuzvbKRc8XzFCdmtUOlm95WTBzIDbDiLJ-na3...</td>\n",
       "      <td>Escoffery Babatunde</td>\n",
       "      <td>https://play-lh.googleusercontent.com/a/AATXAJ...</td>\n",
       "      <td>Can't pay the ebills if the payments feature d...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>12.5.64086</td>\n",
       "      <td>2022-06-01 10:51:45</td>\n",
       "      <td>Please excuse the inconvenience. The problem r...</td>\n",
       "      <td>2022-06-01 16:14:48</td>\n",
       "      <td>pay bills payments feature doesnt work shows b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0  \\\n",
       "0             0           0   \n",
       "1             1           1   \n",
       "2             2           2   \n",
       "3             3           3   \n",
       "4             4           4   \n",
       "\n",
       "                                            reviewId             userName  \\\n",
       "0  gp:AOqpTOFKM8vyKDl8bQv21U8i2O8m6EdIkpCn8XNWJYj...            yoann mii   \n",
       "1  gp:AOqpTOGpGnFdSoTVSpYBE-XY0td_ZsoQX9lbL_aHZAc...         Pratik Gilda   \n",
       "2  gp:AOqpTOEPnZAw5fgoez35bU4IvWwSKSrfuWkwFs4USPK...       Radosław Kania   \n",
       "3  gp:AOqpTOEFeYQ8CVjsVT-13q2tNJWFbabIEiOupsx5hkb...            adi leist   \n",
       "4  gp:AOqpTOHuzvbKRc8XzFCdmtUOlm95WTBzIDbDiLJ-na3...  Escoffery Babatunde   \n",
       "\n",
       "                                           userImage  \\\n",
       "0  https://play-lh.googleusercontent.com/a/AATXAJ...   \n",
       "1  https://play-lh.googleusercontent.com/a-/AOh14...   \n",
       "2  https://play-lh.googleusercontent.com/a/AATXAJ...   \n",
       "3  https://play-lh.googleusercontent.com/a/AATXAJ...   \n",
       "4  https://play-lh.googleusercontent.com/a/AATXAJ...   \n",
       "\n",
       "                                             content  score  thumbsUpCount  \\\n",
       "0        doesnt work on many phones and full of bugs      1              0   \n",
       "1  Update on 2-june-22: unable to login with acce...      1              1   \n",
       "2  app is very slow. additional app for access in...      3              0   \n",
       "3  Lots of bugs since the last release. 1) QR pay...      1              0   \n",
       "4  Can't pay the ebills if the payments feature d...      2              4   \n",
       "\n",
       "  reviewCreatedVersion                   at  \\\n",
       "0           12.5.64086  2022-06-02 17:06:19   \n",
       "1           12.5.64086  2022-06-02 16:06:12   \n",
       "2           12.5.64086  2022-06-01 22:39:43   \n",
       "3           12.5.64086  2022-06-01 16:59:37   \n",
       "4           12.5.64086  2022-06-01 10:51:45   \n",
       "\n",
       "                                        replyContent            repliedAt  \\\n",
       "0  Thank you for your feedback & please excuse th...  2022-06-03 13:51:16   \n",
       "1  Thank you very much for your patience and plea...  2022-06-03 14:52:29   \n",
       "2  Thank you for your feedback & please excuse th...  2022-06-02 16:19:20   \n",
       "3                                                NaN                  NaN   \n",
       "4  Please excuse the inconvenience. The problem r...  2022-06-01 16:14:48   \n",
       "\n",
       "                                       clean_content  \n",
       "0                            doesnt work phones bugs  \n",
       "1         update june unable login access app update  \n",
       "2             app slow additional app access shocker  \n",
       "3  lots bugs release payments scanning function d...  \n",
       "4  pay bills payments feature doesnt work shows b...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad22613d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.clean_content.values.tolist() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bb02ff",
   "metadata": {},
   "source": [
    "## Tokenize words and cleanup the text\n",
    "\n",
    "Use gensims simple_preprocess(), set deacc=True to remove punctuations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4793a6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7360313f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['doesnt', 'work', 'phones', 'bugs']]\n"
     ]
    }
   ],
   "source": [
    "data_words = list(sent_to_words(data))\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a537c3",
   "metadata": {},
   "source": [
    "## Creating Bigram and Trigram models\n",
    "\n",
    "Bigrams are 2 words frequently occuring together in docuent. Trigrams are 3 words frequently occuring. Many other techniques are explained in part-1 of the blog which are important in NLP pipline, it would be worth your while going through that blog. The 2 arguments for Phrases are min_count and threshold. The higher the values of these parameters , the harder its for a word to be combined to bigram.\n",
    "\n",
    "\n",
    "**Bigrams** are two words frequently occurring together in the document. **Trigrams** are 3 words frequently occurring.\n",
    "\n",
    "Some examples in our example are: ‘front_bumper’, ‘oil_leak’, ‘maryland_college_park’ etc.\n",
    "Gensim’s Phrases model can build and implement the bigrams, trigrams, quadgrams and more. The two important arguments to Phrases are min_count and threshold. The higher the values of these param, the harder it is for words to be combined to bigrams.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f631fc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['doesnt', 'work', 'phones', 'bugs']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d640b0e3",
   "metadata": {},
   "source": [
    "## Remove Stopwords, make bigrams and lemmatize\n",
    "\n",
    "Using lemmatization instead of stemming is a practice which especially pays off in topic modeling because lemmatized words tend to be more human-readable than stemming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f8fe4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23ab0a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25a95358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['work', 'phone', 'bug']]\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39608c3d",
   "metadata": {},
   "source": [
    "## Create Dictionary and Corpus needed for Topic Modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d22ea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary \n",
    "id2word = corpora.Dictionary(data_lemmatized)  \n",
    "# Create Corpus \n",
    "texts = data_lemmatized  \n",
    "# Term Document Frequency \n",
    "corpus = [id2word.doc2bow(text) for text in texts]  \n",
    "# View \n",
    "print(corpus[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3e7017",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0a76a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c561bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  \n",
    "# a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb63a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb6868a",
   "metadata": {},
   "source": [
    "# Using Gensim for LDA\n",
    "\n",
    "I will be using the Latent Dirichlet Allocation (LDA) from Gensim package along with the Mallet’s implementation (via Gensim). Mallet has an efficient implementation of the LDA. It is known to run faster and gives better topics segregation.\n",
    "\n",
    "We will also extract the volume and percentage contribution of each topic to get an idea of how important a topic is.\n",
    "\n",
    "Let’s begin!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd5fb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abf8328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6804e22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca53bef5",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9022a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337e5327",
   "metadata": {},
   "source": [
    "## Create Corpora and Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ec6b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c76492",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30229a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word.doc2bow(data_words[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3410129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Corpus\n",
    "texts = data_words\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "# View\n",
    "print(corpus[:1][0][:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6e034b",
   "metadata": {},
   "source": [
    "## LDA model training\n",
    "\n",
    "- Gensim \n",
    "\n",
    "To keep things simple, we’ll keep all the parameters to default except for inputting the number of topics. For this tutorial, we will build a model with 10 topics where each topic is a combination of keywords, and each keyword contributes a certain weightage to the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d39f94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# number of topics\n",
    "num_topics = 10\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48cfb11",
   "metadata": {},
   "source": [
    "## Analyzing LDA model results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4f303e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "\n",
    "gensimvis.prepare(lda_model, corpus, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1826f49d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "358.398px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
